#!/usr/bin/env python3
"""
é’“é±¼ç½‘ç«™æ£€æµ‹å™¨è®­ç»ƒé…ç½®
é’ˆå¯¹4090æ˜¾å¡ä¼˜åŒ–
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
import transformers
from transformers import AutoModel, AutoTokenizer
import numpy as np
from typing import Dict, List, Any
import yaml

# 4090æ˜¾å¡é…ç½®
DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
GPU_MEMORY = 24  # 4090æ˜¾å­˜24GB
BATCH_SIZE = 64  # æ ¹æ®æ˜¾å­˜è°ƒæ•´
NUM_WORKERS = 8  # å¤šè¿›ç¨‹æ•°æ®åŠ è½½

class PhishingDataset(Dataset):
    """é’“é±¼ç½‘ç«™æ•°æ®é›†"""
    def __init__(self, features: List[Dict[str, Any]], labels: List[int]):
        self.features = features
        self.labels = labels

    def __len__(self):
        return len(self.features)

    def __getitem__(self, idx):
        return {
            'url_features': torch.tensor(self.features[idx]['url_features'], dtype=torch.float),
            'html_features': torch.tensor(self.features[idx]['html_features'], dtype=torch.float),
            'ssl_features': torch.tensor(self.features[idx]['ssl_features'], dtype=torch.float),
            'label': torch.tensor(self.labels[idx], dtype=torch.long)
        }

class AdvancedPhishingDetector(nn.Module):
    """é«˜çº§é’“é±¼ç½‘ç«™æ£€æµ‹å™¨ - é’ˆå¯¹4090ä¼˜åŒ–"""

    def __init__(self, config: Dict[str, Any]):
        super().__init__()

        # URLå¤„ç†åˆ†æ”¯
        self.url_embedding = nn.Sequential(
            nn.Linear(50, 256),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(256, 512),
            nn.ReLU(),
            nn.Dropout(0.3)
        )

        # HTMLç‰¹å¾å¤„ç†åˆ†æ”¯
        self.html_processor = nn.Sequential(
            nn.Linear(100, 256),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(256, 512),
            nn.ReLU(),
            nn.Dropout(0.3)
        )

        # SSLç‰¹å¾å¤„ç†åˆ†æ”¯
        self.ssl_processor = nn.Sequential(
            nn.Linear(20, 128),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(128, 256),
            nn.ReLU(),
            nn.Dropout(0.2)
        )

        # èåˆå±‚
        self.fusion_layer = nn.Sequential(
            nn.Linear(512 + 512 + 256, 1024),
            nn.ReLU(),
            nn.Dropout(0.4),
            nn.Linear(1024, 512),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(256, 2)  # äºŒåˆ†ç±»
        )

        # æ³¨æ„åŠ›æœºåˆ¶
        self.attention = nn.MultiheadAttention(embed_dim=512, num_heads=8)

    def forward(self, url_features, html_features, ssl_features):
        # å¤„ç†å„åˆ†æ”¯ç‰¹å¾
        url_out = self.url_embedding(url_features)
        html_out = self.html_processor(html_features)
        ssl_out = self.ssl_processor(ssl_features)

        # ç‰¹å¾èåˆ
        combined = torch.cat([url_out, html_out, ssl_out], dim=1)

        # æ³¨æ„åŠ›æœºåˆ¶
        attn_out, _ = self.attention(combined.unsqueeze(0), combined.unsqueeze(0), combined.unsqueeze(0))
        attn_out = attn_out.squeeze(0)

        # æœ€ç»ˆåˆ†ç±»
        output = self.fusion_layer(attn_out)

        return output

class TrainingConfig:
    """è®­ç»ƒé…ç½®"""

    def __init__(self):
        self.model_config = {
            'url_feature_dim': 50,
            'html_feature_dim': 100,
            'ssl_feature_dim': 20,
            'hidden_dim': 512,
            'num_classes': 2
        }

        self.training_config = {
            'batch_size': BATCH_SIZE,
            'learning_rate': 0.001,
            'num_epochs': 100,
            'early_stopping_patience': 10,
            'weight_decay': 0.0001,
            'gradient_clip_norm': 1.0,
            'warmup_epochs': 5,
            'mixup_alpha': 0.2,
            'label_smoothing': 0.1
        }

        self.optimizer_config = {
            'type': 'AdamW',
            'lr': 0.001,
            'weight_decay': 0.0001,
            'betas': (0.9, 0.999),
            'eps': 1e-8
        }

        self.scheduler_config = {
            'type': 'CosineAnnealingWarmRestarts',
            'T_0': 10,
            'T_mult': 2,
            'eta_min': 1e-6
        }

        self.augmentation_config = {
            'url_augmentation': True,
            'html_augmentation': True,
            'noise_level': 0.1,
            'dropout_rate': 0.3
        }

        self.multi_gpu_config = {
            'use_data_parallel': torch.cuda.device_count() > 1,
            'num_gpus': torch.cuda.device_count(),
            'sync_batchnorm': True
        }

class Trainer:
    """è®­ç»ƒå™¨ - é’ˆå¯¹4090ä¼˜åŒ–"""

    def __init__(self, config: TrainingConfig):
        self.config = config
        self.device = DEVICE
        self.model = AdvancedPhishingDetector(config.model_config).to(self.device)

        # å¤šGPUæ”¯æŒ
        if config.multi_gpu_config['use_data_parallel']:
            self.model = nn.DataParallel(self.model)

        # ä¼˜åŒ–å™¨
        if config.optimizer_config['type'] == 'AdamW':
            self.optimizer = optim.AdamW(
                self.model.parameters(),
                lr=config.optimizer_config['lr'],
                weight_decay=config.optimizer_config['weight_decay'],
                betas=config.optimizer_config['betas'],
                eps=config.optimizer_config['eps']
            )

        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        if config.scheduler_config['type'] == 'CosineAnnealingWarmRestarts':
            self.scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(
                self.optimizer,
                T_0=config.scheduler_config['T_0'],
                T_mult=config.scheduler_config['T_mult'],
                eta_min=config.scheduler_config['eta_min']
            )

        # æŸå¤±å‡½æ•°
        self.criterion = nn.CrossEntropyLoss(
            label_smoothing=config.training_config['label_smoothing']
        )

        # æ··åˆç²¾åº¦è®­ç»ƒ
        self.scaler = torch.cuda.amp.GradScaler()

    def train_epoch(self, train_loader: DataLoader) -> float:
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        total_loss = 0.0

        for batch_idx, batch in enumerate(train_loader):
            # æ•°æ®ç§»åŠ¨åˆ°GPU
            url_features = batch['url_features'].to(self.device)
            html_features = batch['html_features'].to(self.device)
            ssl_features = batch['ssl_features'].to(self.device)
            labels = batch['label'].to(self.device)

            # æ¢¯åº¦æ¸…é›¶
            self.optimizer.zero_grad()

            # æ··åˆç²¾åº¦è®­ç»ƒ
            with torch.cuda.amp.autocast():
                outputs = self.model(url_features, html_features, ssl_features)
                loss = self.criterion(outputs, labels)

            # åå‘ä¼ æ’­
            self.scaler.scale(loss).backward()

            # æ¢¯åº¦è£å‰ª
            torch.nn.utils.clip_grad_norm_(
                self.model.parameters(),
                self.config.training_config['gradient_clip_norm']
            )

            # ä¼˜åŒ–å™¨æ­¥éª¤
            self.scaler.step(self.optimizer)
            self.scaler.update()

            total_loss += loss.item()

        return total_loss / len(train_loader)

    def validate(self, val_loader: DataLoader) -> Dict[str, float]:
        """éªŒè¯æ¨¡å‹"""
        self.model.eval()
        total_loss = 0.0
        correct = 0
        total = 0

        with torch.no_grad():
            for batch in val_loader:
                url_features = batch['url_features'].to(self.device)
                html_features = batch['html_features'].to(self.device)
                ssl_features = batch['ssl_features'].to(self.device)
                labels = batch['label'].to(self.device)

                outputs = self.model(url_features, html_features, ssl_features)
                loss = self.criterion(outputs, labels)

                total_loss += loss.item()
                _, predicted = outputs.max(1)
                total += labels.size(0)
                correct += predicted.eq(labels).sum().item()

        accuracy = 100. * correct / total
        avg_loss = total_loss / len(val_loader)

        return {
            'val_loss': avg_loss,
            'val_accuracy': accuracy
        }

    def train(self, train_loader: DataLoader, val_loader: DataLoader):
        """å®Œæ•´è®­ç»ƒæµç¨‹"""
        best_val_accuracy = 0.0
        patience_counter = 0

        for epoch in range(self.config.training_config['num_epochs']):
            # è®­ç»ƒ
            train_loss = self.train_epoch(train_loader)

            # éªŒè¯
            val_metrics = self.validate(val_loader)

            # å­¦ä¹ ç‡è°ƒæ•´
            self.scheduler.step()

            # æ‰“å°è¿›åº¦
            print(f'Epoch [{epoch+1}/{self.config.training_config["num_epochs"]}]')
            print(f'Train Loss: {train_loss:.4f}')
            print(f'Val Loss: {val_metrics["val_loss"]:.4f}')
            print(f'Val Accuracy: {val_metrics["val_accuracy"]:.2f}%')
            print(f'LR: {self.scheduler.get_last_lr()[0]:.6f}')
            print('-' * 50)

            # æ—©åœ
            if val_metrics['val_accuracy'] > best_val_accuracy:
                best_val_accuracy = val_metrics['val_accuracy']
                patience_counter = 0
                # ä¿å­˜æœ€ä½³æ¨¡å‹
                torch.save(self.model.state_dict(), 'best_model.pth')
            else:
                patience_counter += 1
                if patience_counter >= self.config.training_config['early_stopping_patience']:
                    print(f'Early stopping at epoch {epoch+1}')
                    break

        print(f'Best validation accuracy: {best_val_accuracy:.2f}%')

def create_training_pipeline():
    """åˆ›å»ºè®­ç»ƒæµæ°´çº¿"""

    # åŠ è½½é…ç½®
    config = TrainingConfig()

    # æ‰“å°GPUä¿¡æ¯
    if torch.cuda.is_available():
        print(f"ä½¿ç”¨GPU: {torch.cuda.get_device_name()}")
        print(f"GPUæ•°é‡: {torch.cuda.device_count()}")
        print(f"GPUå†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")

    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = Trainer(config)

    return trainer

if __name__ == "__main__":
    # åˆ›å»ºè®­ç»ƒæµæ°´çº¿
    trainer = create_training_pipeline()

    print("ğŸš€ é’“é±¼ç½‘ç«™æ£€æµ‹å™¨è®­ç»ƒå™¨å·²å‡†å¤‡å°±ç»ª")
    print("ğŸ“Š é’ˆå¯¹RTX 4090ä¼˜åŒ–")
    print("ğŸ¯ æ”¯æŒæ··åˆç²¾åº¦è®­ç»ƒå’Œå¤šGPUè®­ç»ƒ")
    print("ğŸ“ˆ é›†æˆå…ˆè¿›çš„è®­ç»ƒç­–ç•¥å’Œæ­£åˆ™åŒ–æŠ€æœ¯")