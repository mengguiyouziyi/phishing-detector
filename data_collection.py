#!/usr/bin/env python3
"""
é’“é±¼ç½‘ç«™æ•°æ®æ”¶é›†è„šæœ¬
"""

import pandas as pd
import requests
import numpy as np
from urllib.parse import urlparse
import json
import time
import concurrent.futures
from typing import List, Dict, Any
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class PhishingDataCollector:
    """é’“é±¼ç½‘ç«™æ•°æ®æ”¶é›†å™¨"""

    def __init__(self):
        self.datasets = {
            'uci_phishing': 'https://archive.ics.uci.edu/ml/machine-learning-databases/00379/PhishingData.arff',
            'phishtank': 'https://data.phishtank.com/data/online-valid.json',
            'urlnet': 'https://github.com/mjain0/URLNet/raw/master/URLNet/data/benign_list.txt',
            'kaggle_phishing': 'https://raw.githubusercontent.com/agarwalpooja/Phishing-Website-Detection/master/dataset.csv'
        }

    def download_uci_phishing_data(self) -> pd.DataFrame:
        """ä¸‹è½½UCIé’“é±¼ç½‘ç«™æ•°æ®é›†"""
        try:
            url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00379/PhishingData.arff'
            response = requests.get(url)

            # è§£æARFFæ–‡ä»¶
            lines = response.text.split('\n')
            data_start = False
            data = []

            for line in lines:
                if line.strip().startswith('@data'):
                    data_start = True
                    continue
                if data_start and line.strip():
                    # å¤„ç†æ¯ä¸€è¡Œæ•°æ®
                    row = line.strip().split(',')
                    data.append(row)

            # è½¬æ¢ä¸ºDataFrame
            df = pd.DataFrame(data)
            logger.info(f"UCIæ•°æ®é›†ä¸‹è½½å®Œæˆï¼Œå…± {len(df)} æ¡è®°å½•")
            return df

        except Exception as e:
            logger.error(f"ä¸‹è½½UCIæ•°æ®é›†å¤±è´¥: {e}")
            return pd.DataFrame()

    def download_phishtank_data(self, limit: int = 1000) -> pd.DataFrame:
        """ä¸‹è½½PhishTanké’“é±¼ç½‘ç«™æ•°æ®"""
        try:
            url = 'https://data.phishtank.com/data/online-valid.json'
            response = requests.get(url)
            data = response.json()

            # æå–URLå’Œå…¶ä»–ä¿¡æ¯
            phishing_data = []
            for item in data[:limit]:
                phishing_data.append({
                    'url': item.get('url'),
                    'phish_id': item.get('phish_id'),
                    'target': item.get('target'),
                    'verified': item.get('verified'),
                    'verification_time': item.get('verification_time'),
                    'is_phishing': 1
                })

            df = pd.DataFrame(phishing_data)
            logger.info(f"PhishTankæ•°æ®é›†ä¸‹è½½å®Œæˆï¼Œå…± {len(df)} æ¡è®°å½•")
            return df

        except Exception as e:
            logger.error(f"ä¸‹è½½PhishTankæ•°æ®é›†å¤±è´¥: {e}")
            return pd.DataFrame()

    def download_legitimate_urls(self) -> pd.DataFrame:
        """ä¸‹è½½æ­£å¸¸ç½‘ç«™URL"""
        try:
            # ä»Trancoåˆ—è¡¨è·å–æ­£å¸¸ç½‘ç«™
            url = 'https://tranco-list.eu/download/ZQ100000/100000'
            response = requests.get(url)

            legitimate_urls = []
            for line in response.text.strip().split('\n')[:5000]:  # é™åˆ¶5000ä¸ªæ­£å¸¸ç½‘ç«™
                rank, domain = line.split(',')
                legitimate_urls.append({
                    'url': f'http://{domain}',
                    'domain': domain,
                    'rank': int(rank),
                    'is_phishing': 0
                })

            df = pd.DataFrame(legitimate_urls)
            logger.info(f"æ­£å¸¸ç½‘ç«™æ•°æ®ä¸‹è½½å®Œæˆï¼Œå…± {len(df)} æ¡è®°å½•")
            return df

        except Exception as e:
            logger.error(f"ä¸‹è½½æ­£å¸¸ç½‘ç«™æ•°æ®å¤±è´¥: {e}")
            return pd.DataFrame()

    def generate_synthetic_features(self, url: str) -> Dict[str, Any]:
        """ç”ŸæˆURLç‰¹å¾"""
        try:
            parsed = urlparse(url)
            domain = parsed.netloc

            features = {
                'url_length': len(url),
                'domain_length': len(domain),
                'path_length': len(parsed.path),
                'query_length': len(parsed.query),
                'num_dots': url.count('.'),
                'num_hyphens': url.count('-'),
                'num_underscores': url.count('_'),
                'num_slashes': url.count('/'),
                'num_question_marks': url.count('?'),
                'num_equals': url.count('='),
                'num_at_symbols': url.count('@'),
                'num_ampersands': url.count('&'),
                'num_percent': url.count('%'),
                'has_ip_address': self._has_ip_address(domain),
                'has_https': url.startswith('https://'),
                'num_subdomains': len(domain.split('.')) - 1,
                'url_entropy': self._calculate_entropy(url),
                'domain_entropy': self._calculate_entropy(domain),
                'has_suspicious_words': self._has_suspicious_words(url),
                'num_digits': sum(c.isdigit() for c in url),
                'num_letters': sum(c.isalpha() for c in url),
                'digit_letter_ratio': sum(c.isdigit() for c in url) / max(1, sum(c.isalpha() for c in url)),
                'special_char_ratio': sum(not c.isalnum() for c in url) / len(url),
                'has_login_words': self._has_login_keywords(url),
                'has_bank_words': self._has_bank_keywords(url),
                'has_suspicious_tld': self._has_suspicious_tld(domain),
                'is_shortened_url': self._is_shortened_url(url)
            }

            return features

        except Exception as e:
            logger.error(f"ç”ŸæˆURLç‰¹å¾å¤±è´¥: {e}")
            return {}

    def _has_ip_address(self, domain: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦åŒ…å«IPåœ°å€"""
        import re
        ip_pattern = r'^\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}$'
        return bool(re.match(ip_pattern, domain))

    def _calculate_entropy(self, text: str) -> float:
        """è®¡ç®—ç†µå€¼"""
        if not text:
            return 0.0

        from collections import Counter
        import math

        counts = Counter(text)
        total = len(text)
        entropy = 0.0

        for count in counts.values():
            probability = count / total
            if probability > 0:
                entropy -= probability * math.log2(probability)

        return entropy

    def _has_suspicious_words(self, url: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦åŒ…å«å¯ç–‘è¯æ±‡"""
        suspicious_words = [
            'login', 'signin', 'verify', 'secure', 'account', 'update',
            'confirm', 'banking', 'paypal', 'amazon', 'microsoft',
            'apple', 'google', 'facebook', 'twitter', 'instagram'
        ]

        return any(word.lower() in url.lower() for word in suspicious_words)

    def _has_login_keywords(self, url: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦åŒ…å«ç™»å½•ç›¸å…³å…³é”®è¯"""
        login_keywords = ['login', 'signin', 'auth', 'authentication', 'credential']
        return any(keyword in url.lower() for keyword in login_keywords)

    def _has_bank_keywords(self, url: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦åŒ…å«é“¶è¡Œç›¸å…³å…³é”®è¯"""
        bank_keywords = ['bank', 'payment', 'credit', 'debit', 'transfer', 'wire']
        return any(keyword in url.lower() for keyword in bank_keywords)

    def _has_suspicious_tld(self, domain: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦åŒ…å«å¯ç–‘é¡¶çº§åŸŸå"""
        suspicious_tlds = ['.tk', '.ml', '.ga', '.cf', '.xyz', '.top', '.click', '.download']
        return any(domain.endswith(tld) for tld in suspicious_tlds)

    def _is_shortened_url(self, url: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦ä¸ºçŸ­é“¾æ¥"""
        shortened_domains = ['bit.ly', 'tinyurl.com', 'short.link', 'goo.gl', 't.co']
        return any(domain in url for domain in shortened_domains)

    def collect_and_process_data(self) -> pd.DataFrame:
        """æ”¶é›†å¹¶å¤„ç†æ‰€æœ‰æ•°æ®"""
        logger.info("å¼€å§‹æ”¶é›†é’“é±¼ç½‘ç«™æ•°æ®...")

        # ä¸‹è½½å„æ•°æ®é›†
        uci_data = self.download_uci_phishing_data()
        phishtank_data = self.download_phishtank_data(limit=2000)
        legitimate_data = self.download_legitimate_urls()

        # åˆå¹¶æ•°æ®
        all_data = []

        # å¤„ç†é’“é±¼ç½‘ç«™æ•°æ®
        if not phishtank_data.empty:
            for _, row in phishtank_data.iterrows():
                features = self.generate_synthetic_features(row['url'])
                features['is_phishing'] = 1
                features['source'] = 'phishtank'
                all_data.append(features)

        # å¤„ç†æ­£å¸¸ç½‘ç«™æ•°æ®
        if not legitimate_data.empty:
            for _, row in legitimate_data.iterrows():
                features = self.generate_synthetic_features(row['url'])
                features['is_phishing'] = 0
                features['source'] = 'tranco'
                all_data.append(features)

        # è½¬æ¢ä¸ºDataFrame
        df = pd.DataFrame(all_data)

        # æ•°æ®æ¸…æ´—
        df = df.dropna()
        df = df[df['url_length'] > 0]

        # ä¿å­˜æ•°æ®
        df.to_csv('phishing_dataset.csv', index=False)
        df.to_json('phishing_dataset.json', orient='records', indent=2)

        logger.info(f"æ•°æ®æ”¶é›†å®Œæˆï¼Œå…± {len(df)} æ¡è®°å½•")
        logger.info(f"é’“é±¼ç½‘ç«™: {len(df[df['is_phishing'] == 1])}")
        logger.info(f"æ­£å¸¸ç½‘ç«™: {len(df[df['is_phishing'] == 0])}")

        return df

    def get_dataset_statistics(self, df: pd.DataFrame) -> Dict[str, Any]:
        """è·å–æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯"""
        stats = {
            'total_samples': len(df),
            'phishing_samples': len(df[df['is_phishing'] == 1]),
            'legitimate_samples': len(df[df['is_phishing'] == 0]),
            'phishing_ratio': len(df[df['is_phishing'] == 1]) / len(df),
            'feature_columns': [col for col in df.columns if col not in ['is_phishing', 'source']],
            'missing_values': df.isnull().sum().sum(),
            'data_sources': df['source'].value_counts().to_dict() if 'source' in df.columns else {}
        }

        return stats

if __name__ == "__main__":
    collector = PhishingDataCollector()
    dataset = collector.collect_and_process_data()

    if not dataset.empty:
        stats = collector.get_dataset_statistics(dataset)
        print("ğŸ“Š æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯:")
        print(json.dumps(stats, indent=2, ensure_ascii=False))

        # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
        with open('dataset_statistics.json', 'w', encoding='utf-8') as f:
            json.dump(stats, f, indent=2, ensure_ascii=False)

        print("âœ… æ•°æ®æ”¶é›†å®Œæˆ!")
        print("ğŸ“ æ•°æ®æ–‡ä»¶: phishing_dataset.csv")
        print("ğŸ“Š ç»Ÿè®¡ä¿¡æ¯: dataset_statistics.json")
    else:
        print("âŒ æ•°æ®æ”¶é›†å¤±è´¥!")